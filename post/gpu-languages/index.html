<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="author" content="Christophe Gyurgyik">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Christophe Gyurgyik | PhD Student, Stanford University</title>

  <link rel="icon" href="/flower.svg" type="image/svg+xml">

  <link rel="stylesheet" href=https://cgyurgyik.github.io/css/normalize.css>
  <link rel="stylesheet" href=https://cgyurgyik.github.io/css/default.css>
  <link rel="stylesheet" href="https://pro.fontawesome.com/releases/v5.10.0/css/all.css" integrity="sha384-AYmEC3Yw5cVb3ZcuHtOA93w35dYTsvhLPVnYs9eStHfGJvOvKxVfELGroGkvsg+p" crossorigin="anonymous"/>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous"/>

  
  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://cgyurgyik.github.io/post/atom.xml">
  

</head>

<body>
  <header>
    <h1 class="name"><a class="unstyled-link" href="/">Christophe Gyurgyik</a></h1>
    <nav>
      <a class="unstyled-link nav-item" href="/#publications">Publications</a>
      <a class="unstyled-link nav-item" href="/#posts">Posts</a>
      <a class="unstyled-link nav-item" href="/files/resume.pdf">CV</a>
    </nav>
  </header>
  <section class="section">
    <div class="container">
      
<div class="post-header">
<h1 class="title">
  (draft) The GPU Programming Language Triangle
</h1>
<div class="date">
Subjective categorization of GPU languages.
</div>
<p class="date">April 11, 2024</p>
</div>

<div class="post">
<p>Countless programming languages exist today to run fast (and optimistically safe) programs on a GPU. These range from languages in the C family with directives to (mostly) functional languages that veil the low-level details of the hardware. A GPU excels at executing tasks that share a common principal: massive parallelelism. This includes scientific computing, graphics processing, and machine learning. </p>
<p>If the task cannot be parallelized sufficiently, then a CPU wins - it is the jack-of-all-trades of computer hardware. Many programming languages exist today to harness the potential of a CPU. Typically, these languages need to make trade-offs between three aspects: <em>safety</em>, <em>performance</em>, and <em>productivity</em>. This is depicted as a triangle, where the three aforementioned characterizations are placed at the vertices. Then, programming languages are placed somewhere along (or even in) the triangle to demonstrate what influenced their design.</p>
<img src= "/files/images/cpu-triangle.png" alt="cpu-triangle" style="border: 2px solid black;" width="50%">
<p>We define <em>safety</em> as a measurement of how correct the program is, for some definiton of correct. This definition could be “memory accesses are always legal” or “this program is formally proven to be correct under some set of axioms.” We use <em>performance</em> to quantify the amount of work accomplished by the program. Typically, as a language provides more and more abstractions to obfuscate the complexity of a CPU, it will emit less performant code. Contrary to what one might think, performance might not always be a primary concern. Running a program in 0.1 seconds versus 0.01 seconds is a magnitude of order difference, and yet likely unheeded for a student who wants to plot a graph. Lastly, <em>productivity</em> is a function of the additional cognitive exertion required to reason about a program and it’s objective(s). Even putting aside the grandiose debate on syntax, this is likely the most subjective. For example, a CPU performance engineer might claim C as the most productive language because it can easily be mapped to the compiled machine code. On the other hand, a Machine Learning (ML) scientist would avow Python is their tool of choice given the wide array of ML frameworks available - a model can be written in 10 lines of code instead of 10,000! There is no ubiquity in the definition of <em>productivity</em>, and that’s OK, so long as we characterize it before arguing one language is more productive than another.</p>
<p>This post isn’t about CPU programming languages, however. Instead, we’ll focus on programming languages that can, by means of compilation or interpretation, run on a GPU. Many such languages exist, e.g., <a rel="noopener nofollow noreferrer" target="_blank" href="http://learningsys.org/nips17/assets/papers/paper_16.pdf">CuPy</a> is a GPU-powered version of Numpy, which allows users to ignore benign terms like “block size” and “thread index”. The <a rel="noopener nofollow noreferrer" target="_blank" href="https://docs.nvidia.com/cuda/">CUDA C language</a> is a performance engineer’s best friend (for NVIDIA GPUs, at least); it has perilous control over instruction selection for compute-bound kernels and the memory hierarchy for memory-bound kernels. In between these exist a range of languages, that attempt to provide some equilibrium between <em>safety</em>, <em>performance</em>, and <em>productivity</em>.</p>
<h2 id="the-of-gpu-programming-languages">The △ of GPU Programming Languages</h2>
<p>GPUs are specialists - they perform well for embarrassingly parallel problems. The definition of <em>safety</em> remains unchanged: our programs should produce a correct output, for some definition of correct. On the other hand, <em>performance</em> is a key component of GPU programming languages. This is exemplified by the escape hatches even high-level GPU languages provide to squeeze out more performance, e.g., CuPy, a high-level Python framework, provides an API to access low-level CUDA instructions and <a rel="noopener nofollow noreferrer" target="_blank" href="https://futhark-lang.org/publications/fhpc16.pdf">Futhark</a>, a purely functional array language, has additional semantics for in-place array updates. Therefore, we’ll assume that all GPU languages want to produce performant code, and, if it cannot, it is not useful. We’ll instead define our second vertex as <em>versatility</em>, a metric of how much control a user has over the GPU. Intuitively, writing directly in PTX would provide the most versatility. A more versatile language can do everything a less versatile language can do, at the cost of verbosity and complexity. Moreover, a more versatile language will always be able to write at least as performant programs as less versatile languages. Lastly, we define <em>productivity</em> as a function of the cognitive exertion required to reason about the GPU programming model. Mental capacity is finite; ideally one could focus solely on the algorithm and not have to think about the innards of a GPU.</p>
<p>We’ve now defined the three vertices of a GPU programming language triangle. We have a selection of languages spanning both industry and research, which are specific to GPU hardware: <a rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/pdf/2305.03448.pdf">Descend</a>, Triton, Futhark, CuPy, <a rel="noopener nofollow noreferrer" target="_blank" href="https://numba.pydata.org/numba-doc/latest/cuda/index.html">Numba</a>, CUDA, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.khronos.org/opencl/">OpenCL</a>, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://rocm.docs.amd.com/projects/HIP/en/latest/">HIP</a>. Consider the new triangle below:</p>
<img src= "/files/images/gpu-triangle.png" alt="gpu-triangle" style="border: 1px solid black;" with="120%">
<p>These placements assume someone is trying to write efficient GPU kernels, and we acknowledge they are entirely subjective. We’ll spend the rest of this post providing examples of different languages, and finish with a slightly deeper dive into the Triton language.</p>
<h2 id="a-few-examples">A Few Examples</h2>
<p>In an attempt to justify the language placements above, we provide a few examples of matrix multiplication <code>C = A @ B</code>, the defacto standard for high performance computing. Consider the code sample below, written in Futhark:</p>
<pre data-lang="haskell" style="background-color:#2b303b;color:#c0c5ce;" class="language-haskell "><code class="language-haskell" data-lang="haskell"><span style="color:#65737e;">-- (Written in Futhark v0.25.15)
</span><span>def matmul [i][k][j] (</span><span style="color:#d08770;">A</span><span>: [i][k]i32) (</span><span style="color:#d08770;">B</span><span>: [k][j]i32) : [i][j]i32 =
</span><span>  map (\</span><span style="color:#d08770;">Ai </span><span>-&gt;
</span><span>    map (\</span><span style="color:#d08770;">Bj </span><span>-&gt;
</span><span>      reduce </span><span style="color:#8fa1b3;">(+) </span><span style="color:#d08770;">0</span><span> (map \(x, y) -&gt; x * y (zip </span><span style="color:#d08770;">Ai Bj</span><span>)))
</span><span>        (transpose </span><span style="color:#d08770;">B</span><span>)
</span><span>  ) 
</span><span>    </span><span style="color:#d08770;">A
</span></code></pre>
<p>This programming language does not allude to threads, blocks, or memory - instead, the user relies entirely upon the compiler to ensure the code is efficient. </p>
<p>Conversely, Descend is a “safe-by-construction” imperative language. Inspired by Rust, Descend guarantees legal CPU and GPU memory management at compile time by tracking <em>ownership</em> and <em>lifetimes</em>. Following the previous example, we write matrix multiply in Descend:</p>
<pre data-lang="rust" style="background-color:#2b303b;color:#c0c5ce;" class="language-rust "><code class="language-rust" data-lang="rust"><span style="color:#65737e;">// (Descend does not have releases; written on 12 April 2024.)
</span><span style="color:#b48ead;">fn </span><span style="color:#8fa1b3;">matmul</span><span>&lt;BLOCK: nat, i: nat, j: nat, k: nat, r: prv&gt;(
</span><span>    </span><span style="color:#bf616a;">A</span><span>: &amp;r shrd gpu.global [</span><span style="color:#b48ead;">i32</span><span>; i*k],
</span><span>    </span><span style="color:#bf616a;">B</span><span>: &amp;r shrd gpu.global [</span><span style="color:#b48ead;">i32</span><span>; k*j],
</span><span>    </span><span style="color:#bf616a;">C</span><span>: &amp;r uniq gpu.global [</span><span style="color:#b48ead;">i32</span><span>; i*j]
</span><span>) -[grid: gpu.grid&lt;XY&lt;j</span><span style="background-color:#bf616a;color:#2b303b;">/</span><span>BLOCK, i</span><span style="background-color:#bf616a;color:#2b303b;">/</span><span style="color:#d08770;">BLOCK</span><span>&gt;, XY&lt;BLOCK, BLOCK&gt;&gt;] -&gt; () {
</span><span>    </span><span style="color:#96b5b4;">sched</span><span>(Y) block_row in grid {
</span><span>        </span><span style="color:#96b5b4;">sched</span><span>(X) block in block_row {
</span><span>            </span><span style="color:#96b5b4;">sched</span><span>(Y) thread_row in block {
</span><span>                </span><span style="color:#96b5b4;">sched</span><span>(X) thread in thread_row {
</span><span>                    </span><span style="color:#b48ead;">let</span><span> Ai = &amp;shrd (*A)
</span><span>                        .to_view
</span><span>                        .grp::&lt;k&gt;
</span><span>                        .grp::&lt;BLOCK&gt;[[block_row]][[thread_row]];
</span><span>                    </span><span style="color:#b48ead;">let</span><span> Bj = &amp;shrd (*B)
</span><span>                        .to_view
</span><span>                        .grp::&lt;j&gt;
</span><span>                        .transp
</span><span>                        .grp::&lt;BLOCK&gt;[[block]][[thread]];
</span><span>                    </span><span style="color:#b48ead;">let mut</span><span> Cij = &amp;uniq (*C)
</span><span>                        .to_view
</span><span>                        .grp::&lt;j&gt;
</span><span>                        .grp::&lt;BLOCK&gt;[[block_row]]
</span><span>                        .transp
</span><span>                        .grp::&lt;BLOCK&gt;[[block]]
</span><span>                        .transp[[thread_row]][[thread]];
</span><span>                    
</span><span>                    </span><span style="color:#b48ead;">let mut</span><span> sum = </span><span style="color:#d08770;">0</span><span>;
</span><span>                    </span><span style="color:#b48ead;">for</span><span> e in </span><span style="color:#d08770;">0</span><span>..k { 
</span><span>                      sum = sum + (*Ai)[e] * (*Bj)[e] 
</span><span>                    };
</span><span>                    *Cij = sum
</span><span>                }
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Lastly, we write matrix multiplication in CUDA, which follows the Single Instruction Multiple Thread (SIMT) programming paradigm. This low-level language can easily be mapped to the generated PTX, which shows how versatile it really is. Consider the naive square matrix multiplication below, where each dimension has size <code>n</code>:</p>
<pre data-lang="cpp" style="background-color:#2b303b;color:#c0c5ce;" class="language-cpp "><code class="language-cpp" data-lang="cpp"><span>__global__ </span><span style="color:#b48ead;">void </span><span style="color:#8fa1b3;">matmul</span><span>(</span><span style="color:#b48ead;">const int </span><span>*</span><span style="color:#bf616a;">A</span><span>, </span><span style="color:#b48ead;">const int </span><span>*</span><span style="color:#bf616a;">B</span><span>, </span><span style="color:#b48ead;">int </span><span>*</span><span style="color:#bf616a;">C</span><span>, </span><span style="color:#b48ead;">int </span><span style="color:#bf616a;">n</span><span>) {
</span><span>  </span><span style="color:#b48ead;">int</span><span> Ai = blockIdx.</span><span style="color:#bf616a;">y </span><span>* blockDim.</span><span style="color:#bf616a;">y </span><span>+ threadIdx.</span><span style="color:#bf616a;">y</span><span>;
</span><span>  </span><span style="color:#b48ead;">int</span><span> Bj = blockIdx.</span><span style="color:#bf616a;">x </span><span>* blockDim.</span><span style="color:#bf616a;">x </span><span>+ threadIdx.</span><span style="color:#bf616a;">x</span><span>;
</span><span>
</span><span>  </span><span style="color:#b48ead;">int</span><span> temporary = </span><span style="color:#d08770;">0</span><span>;
</span><span>  </span><span style="color:#b48ead;">for </span><span>(</span><span style="color:#b48ead;">int</span><span> k = </span><span style="color:#d08770;">0</span><span>; k &lt; n; k++) {
</span><span>    temporary += A[Ai * n + k] * B[Bj + n * k];
</span><span>  }
</span><span>  C[Ai * n + Bj] = temporary;
</span><span>}
</span></code></pre>
<p>In the example above, each thread loads one row of A and one column of B from global memory, performs an inner product, and stores the result to C. This naive implementation is memory-bound, i.e., no matter how fast the additions and multiplies occur, we will always be waiting on data movement. We can mitigate the overhead of global memory by using a lower-latency memory: shared memory. Note that this will require significant changes to the structure of our code: we need to rewrite loop bounds, update indices, synchronize threads, etc. This is demonstrated in the example below:</p>
<pre data-lang="cpp" style="background-color:#2b303b;color:#c0c5ce;" class="language-cpp "><code class="language-cpp" data-lang="cpp"><span>__global__ </span><span style="color:#b48ead;">void </span><span style="color:#8fa1b3;">matmul</span><span>(</span><span style="color:#b48ead;">const int </span><span>*</span><span style="color:#bf616a;">A</span><span>, </span><span style="color:#b48ead;">const int </span><span>*</span><span style="color:#bf616a;">B</span><span>, </span><span style="color:#b48ead;">int </span><span>*</span><span style="color:#bf616a;">C</span><span>, </span><span style="color:#b48ead;">int </span><span style="color:#bf616a;">n</span><span>) {
</span><span>  </span><span style="color:#b48ead;">int</span><span> Ai = blockIdx.</span><span style="color:#bf616a;">y </span><span>* blockDim.</span><span style="color:#bf616a;">y </span><span>+ threadIdx.</span><span style="color:#bf616a;">y</span><span>;
</span><span>  </span><span style="color:#b48ead;">int</span><span> Bj = blockIdx.</span><span style="color:#bf616a;">x </span><span>* blockDim.</span><span style="color:#bf616a;">x </span><span>+ threadIdx.</span><span style="color:#bf616a;">x</span><span>;
</span><span>
</span><span>  __shared__ </span><span style="color:#b48ead;">int</span><span> shA[n];
</span><span>  __shared__ </span><span style="color:#b48ead;">int</span><span> shB[n];
</span><span>
</span><span>  </span><span style="color:#b48ead;">int</span><span> temporary = </span><span style="color:#d08770;">0</span><span>;
</span><span>  </span><span style="color:#b48ead;">for </span><span>(</span><span style="color:#b48ead;">int</span><span> i = </span><span style="color:#d08770;">0</span><span>; i &lt; n; i += blockDim.</span><span style="color:#bf616a;">x</span><span>) {
</span><span>    shA[threadIdx.</span><span style="color:#bf616a;">y </span><span>* blockDim.</span><span style="color:#bf616a;">x </span><span>+ threadIdx.</span><span style="color:#bf616a;">x</span><span>] =
</span><span>          A[Ai * n + i + threadIdx.</span><span style="color:#bf616a;">x    </span><span>];
</span><span>    shB[threadIdx.</span><span style="color:#bf616a;">y </span><span>* blockDim.</span><span style="color:#bf616a;">x </span><span>+ threadIdx.</span><span style="color:#bf616a;">x</span><span>] =
</span><span>          B[Bj + n * i + threadIdx.</span><span style="color:#bf616a;">y </span><span>* n];
</span><span>
</span><span>    </span><span style="color:#65737e;">// Wait until all threads finish loading to shared memory.
</span><span>    </span><span style="color:#bf616a;">__syncthreads</span><span>();  
</span><span>    </span><span style="color:#b48ead;">for </span><span>(</span><span style="color:#b48ead;">int</span><span> j = </span><span style="color:#d08770;">0</span><span>; j &lt; blockDim.</span><span style="color:#bf616a;">x</span><span>; j++) {
</span><span>      temporary +=
</span><span>        shA[threadIdx.</span><span style="color:#bf616a;">y </span><span>* blockDim.</span><span style="color:#bf616a;">x </span><span>+ j] * 
</span><span>        shB[j * blockDim.</span><span style="color:#bf616a;">x </span><span>+ threadIdx.</span><span style="color:#bf616a;">x</span><span>];
</span><span>    }
</span><span>    </span><span style="color:#65737e;">// Wait until all threads finish reading from shared memory.
</span><span>    </span><span style="color:#bf616a;">__syncthreads</span><span>();  
</span><span>  }
</span><span>  C[Ai * n + Bj] = temporary;
</span><span>}
</span></code></pre>
<p>This is just one of many optimizations to get a performant matrix multiplication kernel. We aren’t even considering multiple-level tiling, using Tensor Cores, etc. In some utopian setting, we wouldn’t <em>need</em> to, the compiler would do this automatically for us; in comes <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Triton</a>.</p>
<h2 id="what-is-triton">What is Triton?</h2>
<p>Triton is an imperative language and compiler stack to simplify the arduous process of writing GPU kernels. Antithetical to the SIMT model, users write programs that load, compute upon, and store <em>blocks</em> of memory. These blocks are accessed via a pointer interface. Then, the compiler automatically handles optimizations such as multi-threading, using fast memory, tensor cores, etc. So, the user must handle the outermost level of tiling, via loads and stores to global memory, and then the compiler handles the rest. The <a rel="noopener nofollow noreferrer" target="_blank" href="https://triton-lang.org/main/">Triton website</a> provides a comprehensive overview of the language with some concrete examples. For completeness, we provide a Matrix Multiply example:</p>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>@triton.</span><span style="color:#bf616a;">jit
</span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">matmul_kernel</span><span>(
</span><span>        </span><span style="color:#bf616a;">A</span><span>, </span><span style="color:#bf616a;">B</span><span>, </span><span style="color:#bf616a;">C</span><span>,                    </span><span style="color:#65737e;"># Pointers to matrices
</span><span>        </span><span style="color:#bf616a;">M</span><span>, </span><span style="color:#bf616a;">N</span><span>, </span><span style="color:#bf616a;">K</span><span>,                    </span><span style="color:#65737e;"># Matrix dimensions
</span><span>        </span><span style="color:#bf616a;">stride_am</span><span>, </span><span style="color:#bf616a;">stride_ak</span><span>,       </span><span style="color:#65737e;"># Stride parameters
</span><span>        </span><span style="color:#bf616a;">stride_bk</span><span>, </span><span style="color:#bf616a;">stride_bn</span><span>,  
</span><span>        </span><span style="color:#bf616a;">stride_cm</span><span>, </span><span style="color:#bf616a;">stride_cn</span><span>,
</span><span>        </span><span style="color:#bf616a;">BLOCK_SIZE_M</span><span>: tl.constexpr, </span><span style="color:#65737e;"># Meta-parameters
</span><span>        </span><span style="color:#bf616a;">BLOCK_SIZE_N</span><span>: tl.constexpr, 
</span><span>        </span><span style="color:#bf616a;">BLOCK_SIZE_K</span><span>: tl.constexpr, 
</span><span>        </span><span style="color:#bf616a;">GROUP_SIZE_M</span><span>: tl.constexpr, 
</span><span>):
</span><span>    </span><span style="color:#65737e;">&quot;&quot;&quot;
</span><span style="color:#65737e;">    Kernel for computing the matmul C = A @ B.
</span><span style="color:#65737e;">    A has shape (M, K), B has shape (K, N) and C has shape (M, N)
</span><span style="color:#65737e;">    &quot;&quot;&quot;
</span><span>    pid = tl.</span><span style="color:#bf616a;">program_id</span><span>(</span><span style="color:#bf616a;">axis</span><span>=</span><span style="color:#d08770;">0</span><span>)
</span><span>    num_pid_m = tl.</span><span style="color:#bf616a;">cdiv</span><span>(M, </span><span style="color:#bf616a;">BLOCK_SIZE_M</span><span>)
</span><span>    num_pid_n = tl.</span><span style="color:#bf616a;">cdiv</span><span>(N, </span><span style="color:#bf616a;">BLOCK_SIZE_N</span><span>)
</span><span>    num_pid_in_group = </span><span style="color:#bf616a;">GROUP_SIZE_M </span><span>* num_pid_n
</span><span>    group_id = pid // num_pid_in_group
</span><span>    first_pid_m = group_id * </span><span style="color:#bf616a;">GROUP_SIZE_M
</span><span>    group_size_m = </span><span style="color:#96b5b4;">min</span><span>(num_pid_m - first_pid_m, </span><span style="color:#bf616a;">GROUP_SIZE_M</span><span>)
</span><span>    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)
</span><span>    pid_n = (pid % num_pid_in_group) // group_size_m
</span><span>
</span><span>    offs_am = (pid_m * </span><span style="color:#bf616a;">BLOCK_SIZE_M </span><span>+ tl.</span><span style="color:#bf616a;">arange</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#bf616a;">BLOCK_SIZE_M</span><span>)) % M
</span><span>    offs_bn = (pid_n * </span><span style="color:#bf616a;">BLOCK_SIZE_N </span><span>+ tl.</span><span style="color:#bf616a;">arange</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#bf616a;">BLOCK_SIZE_N</span><span>)) % N
</span><span>    offs_k = tl.</span><span style="color:#bf616a;">arange</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#bf616a;">BLOCK_SIZE_K</span><span>)
</span><span>    As = A + \
</span><span>         (offs_am[:, </span><span style="color:#d08770;">None</span><span>] * stride_am + offs_k[</span><span style="color:#d08770;">None</span><span>, :] * stride_ak)
</span><span>    Bs = B + \
</span><span>         (offs_k[:, </span><span style="color:#d08770;">None</span><span>] * stride_bk + offs_bn[</span><span style="color:#d08770;">None</span><span>, :] * stride_bn)
</span><span>
</span><span>    accumulator = tl.</span><span style="color:#bf616a;">zeros</span><span>(
</span><span>      (</span><span style="color:#bf616a;">BLOCK_SIZE_M</span><span>, </span><span style="color:#bf616a;">BLOCK_SIZE_N</span><span>), 
</span><span>      </span><span style="color:#bf616a;">dtype</span><span>=tl.float32
</span><span>    )
</span><span>    </span><span style="color:#b48ead;">for </span><span>k </span><span style="color:#b48ead;">in </span><span style="color:#96b5b4;">range</span><span>(</span><span style="color:#d08770;">0</span><span>, tl.</span><span style="color:#bf616a;">cdiv</span><span>(K, </span><span style="color:#bf616a;">BLOCK_SIZE_K</span><span>)):
</span><span>        </span><span style="color:#65737e;"># Load the next block of A and B.
</span><span>        a = tl.</span><span style="color:#bf616a;">load</span><span>(
</span><span>          As, 
</span><span>          </span><span style="color:#bf616a;">mask</span><span>=offs_k[</span><span style="color:#d08770;">None</span><span>, :] &lt; K - k * </span><span style="color:#bf616a;">BLOCK_SIZE_K</span><span>, 
</span><span>          </span><span style="color:#bf616a;">other</span><span>=</span><span style="color:#d08770;">0.0
</span><span>        )
</span><span>        b = tl.</span><span style="color:#bf616a;">load</span><span>(
</span><span>          Bs, 
</span><span>          </span><span style="color:#bf616a;">mask</span><span>=offs_k[:, </span><span style="color:#d08770;">None</span><span>] &lt; K - k * </span><span style="color:#bf616a;">BLOCK_SIZE_K</span><span>, 
</span><span>          </span><span style="color:#bf616a;">other</span><span>=</span><span style="color:#d08770;">0.0
</span><span>        )
</span><span>        accumulator = tl.</span><span style="color:#bf616a;">dot</span><span>(a, b, accumulator)
</span><span>        As += </span><span style="color:#bf616a;">BLOCK_SIZE_K </span><span>* stride_ak
</span><span>        Bs += </span><span style="color:#bf616a;">BLOCK_SIZE_K </span><span>* stride_bk
</span><span>
</span><span>    offs_cm = pid_m * </span><span style="color:#bf616a;">BLOCK_SIZE_M </span><span>+ tl.</span><span style="color:#bf616a;">arange</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#bf616a;">BLOCK_SIZE_M</span><span>)
</span><span>    offs_cn = pid_n * </span><span style="color:#bf616a;">BLOCK_SIZE_N </span><span>+ tl.</span><span style="color:#bf616a;">arange</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#bf616a;">BLOCK_SIZE_N</span><span>)
</span><span>    Cs = C + stride_cm * offs_cm[:, </span><span style="color:#d08770;">None</span><span>] + stride_cn * offs_cn[</span><span style="color:#d08770;">None</span><span>, :]
</span><span>    to_store = accumulator.</span><span style="color:#bf616a;">to</span><span>(tl.float16)
</span><span>    c_mask = (offs_cm[:, </span><span style="color:#d08770;">None</span><span>] &lt; M) &amp; (offs_cn[</span><span style="color:#d08770;">None</span><span>, :] &lt; N)
</span><span>    tl.</span><span style="color:#bf616a;">store</span><span>(
</span><span>      Cs, 
</span><span>      to_store, 
</span><span>      </span><span style="color:#bf616a;">mask</span><span>=c_mask
</span><span>    )
</span></code></pre>
<h3 id="who-uses-it">Who uses it?</h3>
<p>The Triton language and compiler stack is currently open source under <a rel="noopener nofollow noreferrer" target="_blank" href="https://openai.com/research/triton">OpenAI</a>. Additionally, <a rel="noopener nofollow noreferrer" target="_blank" href="https://pytorch.org/assets/pytorch2-2.pdf">PyTorch 2.0</a> translates PyTorch programs into Triton in its new compiler backend, TorchInductor. Lastly, <a rel="noopener nofollow noreferrer" target="_blank" href="https://research.google/pubs/compiling-machine-learning-programs-via-high-level-tracing/">JAX</a> uses Triton as a GPU backend target for its new kernel programming model, <a rel="noopener nofollow noreferrer" target="_blank" href="https://jax.readthedocs.io/en/latest/pallas/index.html">Pallas</a>.</p>
<h3 id="strengths">Strengths</h3>
<p>Writing fast GPU programs is hard. For a memory-bound kernel, one must consider, at a minimum, the following:</p>
<ul>
<li>Memory coalescing (in global memory): thread access patterns are important to ensure we minimize the number of fetches.</li>
<li>Memory hierarchy (global $\rightarrow$ shared $\rightarrow$ registers). Using a lower-latency memory is better, but requires synchronization and low-level instructions such as intra-warp shuffles.</li>
<li>Bank conflicts (in shared memory): data structure layout is important to avoid bank conflicts, e.g., Area of Structures (AoS) versus Structure of Arrays (SoA).</li>
</ul>
<p>For a compute-bound kernel such as matrix multiply or convolution, one must map instructions to Tensor Cores. This requires carefully choosing tile size, SM count, etc. Such optimizations are not always trivial; this can be illustrated by the number of NVIDIA Developer blog posts [<a rel="noopener nofollow noreferrer" target="_blank" href="https://developer.nvidia.com/blog/gpu-pro-tip-fast-histograms-using-shared-atomics-maxwell/">1</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/">2</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">3</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">4</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/">5</a>].</p>
<p>By performing block-level data flow analysis, the Triton language can <em>automatically</em> unlock optimizations such as memory coalescing, thread swizzling, pre-fetching, vectorization, instruction selection (e.g. Tensor Core), shared memory allocation and synchronization, and more.</p>
<p>Additionally, Triton provides a few other useful features, such as JIT compilation and auto-tuning support. In general, if you’re not the hottest GPU expert on the block and want to get fast, custom kernels for your application, then this might be the right tool. </p>
<h3 id="weaknesses">Weaknesses</h3>
<p>This is a performance <em>savant</em>’s worst nightmare: we ultimately become victim to a “black box” compiler that we can only hope will optimize our kernel. Worse yet, we don’t really have a way out. In other words, there exist optimizations that aren’t clearly accessible from the high-level abstraction of Triton. So, while Triton may be the right choice for quick iteration, it isn’t necessarily the best choice to squeeze out every drop of performance, i.e., it makes an important trade-off between productivity and versatility.</p>
<p>There is an alluring (yet slightly outdated
<span style="white-space:nowrap">
<label for="sn-1"
       class="margin-toggle sidenote-number">
</label>
</span>
<input type="checkbox"
       id="sn-1"
       class="margin-toggle"/>
<span class="sidenote">
Readers should be aware this was written when Triton 1.0 was released. I have run a few of these example kernels on Triton 2.1.0, and found they have improved since then, e.g., there is no longer unnecessary thread synchronizations in the reduction kernel. However, the post’s objective is still relevant: the Triton compiler is opaque.
</span>
) <a rel="noopener nofollow noreferrer" target="_blank" href="https://fkong.tech/posts/2023-04-23-triton-cuda/">blog post</a> by a senior engineer at NVIDIA, who inspects the emitted code from the Triton compiler, and reverse engineers the PTX back to CUDA (with the assistance of LLMs, no less).</p>
<h2 id="takeaways">Takeaways</h2>
<ul>
<li>GPU languages should always have performance as one of their north stars. If the user didn’t want a fast program, then they could just use a CPU.</li>
<li>There will always be trade-offs between safety, productivity, and versatility. These should provide guidance in the design of your language. For example, Triton is designed for fast iteration on neural network kernels, where “everything is a matrix multiply,” and fusion is an easy way to reduce memory bandwidth.</li>
<li>A compiler provides automatic optimization, but can quickly become a burden for the performance engineer. This is <a rel="noopener nofollow noreferrer" target="_blank" href="http://lua-users.org/lists/lua-l/2011-02/msg00742.html">highlighted</a> by Mike Pall in a thread about the LuaJIT compiler. This is also a driving force behind the <a rel="noopener nofollow noreferrer" target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3519939.3523446">Exo Language</a>, which attempts to <em>externalize</em> the compiler so that the optimizations are transparent.</li>
</ul>

</div>


    </div>
  </section>

<footer>
  &copy;&nbsp;2023 Christophe Gyurgyik &middot; Built using <a href="https://www.getzola.org/">Zola</a> &middot; Adapted from <a href="https://www.rachitnigam.com/">Rachit Nigam</a>
</footer>

</body>

</html>
